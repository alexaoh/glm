---
title: "GLM Practical Sessions, Week 6"
author: "alexaoh"
date: "21.10.21"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "#>", eval = TRUE)
setwd("/home/ajo/gitRepos/glm/Practical2")

library(lattice)
library(car)
library(gridExtra)
library(grid)
library(MASS)
library(survival)
library(multcomp)
library(mvtnorm)
library(tables)
library(HH)
```

## Linear Regression for Cholesterol

```{r}
data <- read.csv2("COL.csv", header = T)
summary(data)
```

## Simple Linear Regression with W - Exercise 1

```{r}
p <- 2
n <- dim(data)[1]

# Fit linear model. 
lm.fit <- lm(C~W, data = data)
summary(lm.fit)
```

### Scatterplot of Points and Regression Line. 

```{r}
# Can be done manually and with a function. 
scatterplot(C~W, smooth = F, data = data)
plot(data[, "W"], data[, "C"], main = "Regression Line for Cholesterol vs. Weight", 
     xlim = c(36, 90), ylim = c(66, 440), xlab = "Weight", ylab = "Cholesterol")
abline(lm.fit, col = 4)
# Could do the plot from above with the scatterplot function above (comes from 'car' package).
```


### Plot Regression Line with Conf. and Pred. Intervals

```{r}
# Plot confidence and prediction intervals with regression line (From package 'HH').
ci.plot(lm.fit)
```


### Plot Predicted Values vs. Residuals

```{r}
# Plot the predicted values vs. residuals. 
plot(predict(lm.fit), resid(lm.fit), main = "Predicted Values vs. Residuals")
abline(h=0, lty = 2)
```


### Plot Standardized/Studentized Residuals

Here: 5 of the points should be outside the lines -2 and 2, since we here have 95\% confidence intervals (2 approximates 1.96) and we have 100 points in the data. We can see that this is the case. 

```{r}
plot(rstandard(lm.fit), main = "Rstandard")
abline(h=c(-2, 0, 2), lty = 2)

plot(rstudent(lm.fit), main = "Rstudent")
abline(h=c(-2, 0, 2), lty = 2)
```


### Diagnostic: Leverage

```{r}
# A line at 0.06 for some reason ? Check code after session!
plot(hatvalues(lm.fit), main= "Leverage (hat-)values")
```

### Diagnostic: Influential observations (dffits, cooks.distance)

Calculate the Cook's distances. 

```{r}
plot(cooks.distance(lm.fit), main = "Cook's Distances")
abline(h=c(0,4/n),lty = 2)
```

Compute dffits (difference of fits). This is the difference between the fits when a point is in or out of the dataset. 

```{r}
plot(dffits(lm.fit), main = "dffits")
abline(h=c(-2*sqrt(p/n), 0, 2*sqrt(p/n)), lty = 2)
```

### Perform a simple regression for each group of age

```{r}
data$AF <- factor(data$A)
sp(C~W|AF,smooth=F,col=1:20, data=data)
```


# Multiple Linear Regression - Exercise 3

```{r}
data <- data[, -5] # Remove AF again. 
scatterplotMatrix(data, smooth = F, diagonal = F)
```

```{r}
lm.fitm <- lm(C~W+A+H, data = data)
summary(lm.fitm)

p <- 4
```

$\hat{\sigma}^2 \approx \text{Residual standard error}^2 = (30.11)^2$. 

### Omnibus test (F-test)

Test the null-model (all coefficients are zero, except for the intercept) vs. our model (at least one of the coefficients are zero).  

### Anova

```{r}
anova(lm.fitm) # Performs the Type-I test. Order of the variables is important. 
```

```{r}
Anova(lm.fitm) # Performs the Type-II test. Order of the variables is NOT important. 
# Can also ask it to compute Type-III test. The order is not important there either. 
```
### Confidence Intervals

```{r}
confint(lm.fitm, level = 0.99)
```

### Prediction

```{r}
CO <- data.frame(cbind(W = c(65, 75, 65), A = c(15, 15, 12), H = c(150, 150, 150)), row.names = 1:3)
predict(lm.fitm, CO, interval = "confidence", level=0.95, se.fit = T) 
# How can it calculate confidence intervals for new predictions (for the mean)?
# Vi plukker ut verdien til konfidensintervallet i tre ulike punkter, derfor har vi tre forskjellige konfidensintervaller. 
```
```{r}
predict(lm.fitm, CO, interval = "prediction", level=0.95, se.fit = T) 
```

## R Diagnostic

```{r}
par(mfrow=c(2,2))
plot(lm.fitm)
```

Then we did some more diagnostics, similar to the ones dones in the simple linear regression above. 

Have a look at the file in Atenea (colesterol-regmultiple.pdf) for all of this + explanations regarding all of the work done in these exercises. 

## Diagnostic: OUTLIERS (rstudent)

```{r}
plot(rstandard(lm.fitm), main = "Rstandard")
abline(h=c(-2,0,2), lty = 2)
```

```{r}

plot(rstudent(lm.fitm), main = "Rstudent")
abline(h=c(-2,0,2), lty = 2)
```

## Diagnostic: LEVERAGE

```{r}
plot(hatvalues(lm.fitm))
abline(h=c(2, 2*mean(hatvalues(lm.fitm))), lty = 2)
abline(h=c(0,3*p/n))
```

## Diagnostic: Influential Values (dffits)

```{r}
plot(cooks.distance(lm.fitm))
abline(h=c(0,4/n),lty= 2)
```

```{r}
plot(dffits(lm.fitm), main="dffits")
abline(h=c(-2*sqrt(p/n), 0, 2*sqrt(p/n)), lty = 2)
```

## Diagnostic: Colinearity

```{r}
vif(lm.fitm)
# Larger VIF signals that the variable is more correlated to the other variables. Linear dependence. 
# Smaller than 1 for VIF is good. Between 1 and 5 is ok. But larger than 5 is not great. 
# This model could/should be simplified, since the variables are correlated. 
```

```{r}
newmod <- lm(C~I(W-(-10+0.5*H))+A+H, data)
summary(newmod)
```

```{r}
vif(newmod)
```

Suppress H, since $p$-value is large. 

```{r}
renewmod <- lm(C~I(W-(-10+0.5*H))+A, data)
summary(renewmod)
```

```{r}
vif(renewmod)
```
